{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:05:08.311797Z",
     "iopub.status.busy": "2022-07-25T11:05:08.311392Z",
     "iopub.status.idle": "2022-07-25T11:05:39.136281Z",
     "shell.execute_reply": "2022-07-25T11:05:39.135070Z",
     "shell.execute_reply.started": "2022-07-25T11:05:08.311707Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install https://huggingface.co/spacy/en_core_web_trf/resolve/main/en_core_web_trf-any-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:05:39.139213Z",
     "iopub.status.busy": "2022-07-25T11:05:39.138609Z",
     "iopub.status.idle": "2022-07-25T11:05:51.948002Z",
     "shell.execute_reply": "2022-07-25T11:05:51.946879Z",
     "shell.execute_reply.started": "2022-07-25T11:05:39.139172Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:05:51.950268Z",
     "iopub.status.busy": "2022-07-25T11:05:51.949910Z",
     "iopub.status.idle": "2022-07-25T11:06:02.161337Z",
     "shell.execute_reply": "2022-07-25T11:06:02.160038Z",
     "shell.execute_reply.started": "2022-07-25T11:05:51.950234Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:06:02.164863Z",
     "iopub.status.busy": "2022-07-25T11:06:02.164542Z",
     "iopub.status.idle": "2022-07-25T11:06:28.164388Z",
     "shell.execute_reply": "2022-07-25T11:06:28.163123Z",
     "shell.execute_reply.started": "2022-07-25T11:06:02.164837Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../input')\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "import itertools\n",
    "from xuexibao.utils.fileUtils import *\n",
    "random.seed('data')\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# lemmatization\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:06:28.167367Z",
     "iopub.status.busy": "2022-07-25T11:06:28.166486Z",
     "iopub.status.idle": "2022-07-25T11:06:28.176837Z",
     "shell.execute_reply": "2022-07-25T11:06:28.175971Z",
     "shell.execute_reply.started": "2022-07-25T11:06:28.167335Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_n_gram(text_list,n):\n",
    "    grams = []\n",
    "    m = len(text_list) - n + 1\n",
    "    for idx in range(m):\n",
    "        gram = text_list[idx:idx+n]\n",
    "        grams.append(gram)\n",
    "    return grams\n",
    "# sample_text = 'so much for making this app such an enjoyable'.split()\n",
    "# get_n_gram(sample_text,1)\n",
    "# get_n_gram(sample_text,2)\n",
    "# get_n_gram(sample_text,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:06:28.179096Z",
     "iopub.status.busy": "2022-07-25T11:06:28.178223Z",
     "iopub.status.idle": "2022-07-25T11:06:28.187372Z",
     "shell.execute_reply": "2022-07-25T11:06:28.186360Z",
     "shell.execute_reply.started": "2022-07-25T11:06:28.179059Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_tag_map(pos_tag):\n",
    "    if pos_tag.startswith('N'):\n",
    "        return 'N'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return 'V'\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return 'J'\n",
    "    elif pos_tag in ['PRP','PRP$','WP','WP$']:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:06:28.189799Z",
     "iopub.status.busy": "2022-07-25T11:06:28.188757Z",
     "iopub.status.idle": "2022-07-25T11:06:28.197806Z",
     "shell.execute_reply": "2022-07-25T11:06:28.197003Z",
     "shell.execute_reply.started": "2022-07-25T11:06:28.189764Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_processing(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sent_lists = []\n",
    "    for idx,sent in enumerate(sentences):\n",
    "        doc = nlp(sent)\n",
    "        # sent_list = [(wnl.lemmatize(w,get_wordnet_pos(t)).lower(),pos_tag_map(t)) for w,t in nltk.pos_tag(nltk.word_tokenize(sent))]\n",
    "        sent_list = [(token.lemma_, pos_tag_map(token.tag_), token.i, token.text)for token in doc]\n",
    "        sent_lists.append(sent_list)\n",
    "    return sent_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:06:28.201015Z",
     "iopub.status.busy": "2022-07-25T11:06:28.200386Z",
     "iopub.status.idle": "2022-07-25T11:06:28.209647Z",
     "shell.execute_reply": "2022-07-25T11:06:28.208752Z",
     "shell.execute_reply.started": "2022-07-25T11:06:28.200980Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "patterns = [['N','N'],\n",
    "           ['V','N'],\n",
    "           ['J','N'],\n",
    "           ['N','CC','N'],\n",
    "           ['J','N','N'],\n",
    "           ['N','N','N'],\n",
    "           ['V','P','N'],\n",
    "           ['V','N','N'],\n",
    "           ['V','J','N'],\n",
    "           ['J','J','N'],\n",
    "           ['N','IN','N'],\n",
    "           ['V','DT','N'],\n",
    "           ['V','N','IN','N'],\n",
    "           ['J','N','N','N'],\n",
    "           ['J','CC','J'],\n",
    "           ['V','IN','J','N'],\n",
    "            ['V','P','J','N'],\n",
    "           ['N','CC','N','N']]\n",
    "\n",
    "patterns_dict = {k+1:v for k,v in enumerate(patterns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:06:28.213173Z",
     "iopub.status.busy": "2022-07-25T11:06:28.211826Z",
     "iopub.status.idle": "2022-07-25T11:06:28.221179Z",
     "shell.execute_reply": "2022-07-25T11:06:28.220258Z",
     "shell.execute_reply.started": "2022-07-25T11:06:28.213144Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_features(features):\n",
    "    delete_list = []\n",
    "    for item in itertools.combinations(features, 2):\n",
    "        if set(item[0][2]).issubset(item[1][2]):\n",
    "            delete_list.append(item[0])\n",
    "        elif set(item[1][2]).issubset(item[0][2]):\n",
    "            delete_list.append(item[1])\n",
    "\n",
    "    new_features = []\n",
    "    \n",
    "    for value in features:\n",
    "        if value not in delete_list:\n",
    "            new_features.append(value)\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:06:28.226696Z",
     "iopub.status.busy": "2022-07-25T11:06:28.226288Z",
     "iopub.status.idle": "2022-07-25T11:06:28.238864Z",
     "shell.execute_reply": "2022-07-25T11:06:28.237870Z",
     "shell.execute_reply.started": "2022-07-25T11:06:28.226667Z"
    }
   },
   "outputs": [],
   "source": [
    "def fiter(lines):\n",
    "    new_lines = []\n",
    "    for i in lines:\n",
    "        if len(i[0])>60 and len(i[0])<600 and \"Incredible\" not in i[0] and i not in new_lines:\n",
    "            new_lines.append(i)\n",
    "    return new_lines\n",
    "def patern(test_lines,name):\n",
    "    json_list = []\n",
    "    for ridx,line in enumerate(test_lines):\n",
    "        tokenized_sents = text_processing(line[0])\n",
    "        for sidx,tokenized_sent in enumerate(tokenized_sents):\n",
    "            result_dict = {'review_id':ridx,'sent_id':sidx,'sent':' '.join([item[-1] for item in tokenized_sent]),}\n",
    "            features = []\n",
    "            gram_2 = get_n_gram(tokenized_sent, 2)\n",
    "            gram_3 = get_n_gram(tokenized_sent, 3)\n",
    "            gram_4 = get_n_gram(tokenized_sent, 4)\n",
    "            for item in gram_2 + gram_3 + gram_4:\n",
    "                pos_list = []\n",
    "                word_list = []\n",
    "                idx_list = []\n",
    "                for word_tuple in item:\n",
    "                    word, pos, idx, _ = word_tuple\n",
    "                    pos_list.append(pos)\n",
    "                    word_list.append(word)\n",
    "                    idx_list.append(idx)\n",
    "                for k,pattern in patterns_dict.items():\n",
    "                    if pos_list == pattern:\n",
    "                        features.append([word_list, pos_list,idx_list])\n",
    "\n",
    "            new_features = delete_features(features)\n",
    "            result_dict['features'] = new_features\n",
    "            json_list.append(result_dict)\n",
    "\n",
    "    print(len(json_list))\n",
    "    dumpJson('./{}.json'.format(name), json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T11:06:28.240671Z",
     "iopub.status.busy": "2022-07-25T11:06:28.240189Z",
     "iopub.status.idle": "2022-07-25T11:08:41.320776Z",
     "shell.execute_reply": "2022-07-25T11:08:41.319644Z",
     "shell.execute_reply.started": "2022-07-25T11:06:28.240637Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed('data')\n",
    "app_list = [\n",
    "\"Facebook_privacy\",\n",
    "\"Instagram_privacy\",\n",
    "\"Spotify_privacy\",\n",
    "\"TikTok_privacy\",\n",
    "\"Twitter_privacy\",\n",
    "\"YouTube_privacy\",\n",
    "\"pocketguard\"\n",
    "]\n",
    "\n",
    "for i in app_list:\n",
    "    lines = readCsvToList('../input/privacy-datas/Privacy_data/{}.csv'.format(i))\n",
    "    a = fiter(lines)[:500]\n",
    "    random.shuffle(a)\n",
    "    test_lines = a[:50]\n",
    "    patern(test_lines,i+'_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
