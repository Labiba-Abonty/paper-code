{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:23:56.530837Z",
     "iopub.status.busy": "2022-08-10T09:23:56.530160Z",
     "iopub.status.idle": "2022-08-10T09:25:36.379715Z",
     "shell.execute_reply": "2022-08-10T09:25:36.378636Z",
     "shell.execute_reply.started": "2022-08-10T09:23:56.530721Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Using spacy.load().\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Importing as module.\n",
    "import en_core_web_trf\n",
    "nlp = en_core_web_trf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:25:36.382668Z",
     "iopub.status.busy": "2022-08-10T09:25:36.381537Z",
     "iopub.status.idle": "2022-08-10T09:25:41.063268Z",
     "shell.execute_reply": "2022-08-10T09:25:41.062187Z",
     "shell.execute_reply.started": "2022-08-10T09:25:36.382632Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:25:41.065496Z",
     "iopub.status.busy": "2022-08-10T09:25:41.065124Z",
     "iopub.status.idle": "2022-08-10T09:25:51.199961Z",
     "shell.execute_reply": "2022-08-10T09:25:51.198633Z",
     "shell.execute_reply.started": "2022-08-10T09:25:41.065461Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:25:51.203768Z",
     "iopub.status.busy": "2022-08-10T09:25:51.203388Z",
     "iopub.status.idle": "2022-08-10T09:26:01.600325Z",
     "shell.execute_reply": "2022-08-10T09:26:01.598980Z",
     "shell.execute_reply.started": "2022-08-10T09:25:51.203735Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:26:01.604049Z",
     "iopub.status.busy": "2022-08-10T09:26:01.603136Z",
     "iopub.status.idle": "2022-08-10T09:26:06.955299Z",
     "shell.execute_reply": "2022-08-10T09:26:06.953999Z",
     "shell.execute_reply.started": "2022-08-10T09:26:01.604012Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import spacy\n",
    "import sys\n",
    "sys.path.append('../input')\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from xuexibao.utils.fileUtils import *\n",
    "random.seed('data')\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "CUSTOM_STOP_WORDS = set(['app','allow','facebook','youtube','twitter','spotify','instagram','tiktok'])\n",
    "NOT_STOP_WORDS = set(['your', 'own','take'])\n",
    "MY_STOP_WORDS = set(stopwords.words('english')).union(set(ENGLISH_STOP_WORDS)).union(CUSTOM_STOP_WORDS) - NOT_STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:26:06.961104Z",
     "iopub.status.busy": "2022-08-10T09:26:06.960710Z",
     "iopub.status.idle": "2022-08-10T09:26:07.006527Z",
     "shell.execute_reply": "2022-08-10T09:26:07.005397Z",
     "shell.execute_reply.started": "2022-08-10T09:26:06.961068Z"
    }
   },
   "outputs": [],
   "source": [
    "def syntactic_match(sentence):\n",
    "    \n",
    "    sentence = sentence.replace('&amp;','and')\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    features['NP'] = features.get('NP',[])\n",
    "    features['CC_NOUN'] = features.get('CC_NOUN',[])\n",
    "    features['CC_ADJ'] = features.get('CC_ADJ',[])\n",
    "    features['DOBJP'] = features.get('DOBJP',[])\n",
    "    features['DOBJ'] = features.get('DOBJ',[])\n",
    "    features['POBJ'] = features.get('POBJ',[])\n",
    "\n",
    "    for np in doc.noun_chunks:\n",
    "        if len(np) > 1:\n",
    "            # print('NP:',np,[item.pos_ for item in np],[np.start,np.end-1])\n",
    "\n",
    "            features['NP'].append([tuple(item.lemma_ for item in np),\n",
    "                                       tuple(item.pos_ for item in np),\n",
    "                                       tuple(item.i for item in np)])\n",
    "\n",
    "    for token in doc:\n",
    "        # print(token.i, token.text, token.pos_, token.dep_, token.head.text, token.head.pos_, [child for child in token.children])\n",
    "        tmp_list = []\n",
    "\n",
    "        if token.pos_ == 'CCONJ' and token.head.pos_ == 'NOUN':\n",
    "            tmp_list.append(token.head)\n",
    "            tmp_list += list(token.head.conjuncts)\n",
    "            ccnoun_list = []\n",
    "            for word in tmp_list:\n",
    "                ccnoun_list.extend(list(filter(lambda item:item.i <= word.i, [child for child in word.children])) + [word])\n",
    "            ccnoun_list.append(token)\n",
    "            ccnoun_list = sorted(ccnoun_list,key = lambda item:item.i)\n",
    "            # print('CC_NOUN:', ccnoun_list, [item.pos_ for item in ccnoun_list], [ccnoun_list[0].i, ccnoun_list[-1].i])\n",
    "\n",
    "            features['CC_NOUN'].append([tuple(item.lemma_ for item in ccnoun_list),\n",
    "                                       tuple(item.pos_ for item in ccnoun_list),\n",
    "                                       tuple(item.i for item in ccnoun_list)])\n",
    "\n",
    "        if token.pos_ == 'CCONJ' and token.head.pos_ == 'ADJ':\n",
    "            tmp_list.append(token.head)\n",
    "            tmp_list += list(token.head.conjuncts)\n",
    "            ccadj_list = []\n",
    "            for word in tmp_list:\n",
    "                ccadj_list.extend([word])\n",
    "            ccadj_list.append(token)\n",
    "            ccadj_list = sorted(ccadj_list,key = lambda item:item.i)\n",
    "            # print('CC_ADJ:', ccadj_list, [item.pos_ for item in ccadj_list], [ccadj_list[0].i, ccadj_list[-1].i])\n",
    "\n",
    "            features['CC_ADJ'].append([tuple(item.lemma_ for item in ccadj_list),\n",
    "                                       tuple(item.pos_ for item in ccadj_list),\n",
    "                                       tuple(item.i for item in ccadj_list)])\n",
    "\n",
    "\n",
    "        if token.dep_ == 'pobj':\n",
    "            pnouns = list(token.conjuncts) + [token]\n",
    "            for pnoun in pnouns:\n",
    "                tmp_list = list(filter(lambda item:item.i <= pnoun.i, token.ancestors)) + [pnoun] + list(filter(lambda item:item.i <= pnoun.i, pnoun.children))\n",
    "                tmp_list = sorted(tmp_list,key = lambda item:item.i)\n",
    "\n",
    "                for idx,word in enumerate(tmp_list[::-1]):\n",
    "                    if word.pos_ == 'VERB':\n",
    "                        \n",
    "                        tmp_list = tmp_list[-(idx+1):]\n",
    "\n",
    "                        # print('POBJ:',tmp_list, [tmp_list[0].i,tmp_list[-1].i])\n",
    "\n",
    "                        features['POBJ'].append([tuple(item.lemma_ for item in tmp_list),\n",
    "                                           tuple(item.pos_ for item in tmp_list),\n",
    "                                           tuple(item.i for item in tmp_list)])\n",
    "                        break\n",
    "                        \n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            tmp_list += [child for child in token.children]\n",
    "            tmp_list.append(token)\n",
    "            tmp_list = list(filter(lambda item:item.i <= token.i, tmp_list))\n",
    "            tmp_list = sorted(tmp_list,key = lambda item:item.i)\n",
    "            tmp_list = [token.head] + tmp_list\n",
    "            if len(tmp_list)>1:\n",
    "                # print('DOBJP:',tmp_list,[item.lemma_ for item in tmp_list],[item.pos_ for item in tmp_list],[tmp_list[0].i,tmp_list[-1].i])\n",
    "\n",
    "                features['DOBJP'].append([tuple(item.lemma_ for item in tmp_list),\n",
    "                                           tuple(item.pos_ for item in tmp_list),\n",
    "                                           tuple(item.i for item in tmp_list)])\n",
    "\n",
    "\n",
    "        if token.dep_ == 'dobj':\n",
    "            tmp_list += [child for child in token.children]\n",
    "            tmp_list.append(token)\n",
    "            tmp_list = list(filter(lambda item:item.i <= token.i, tmp_list))\n",
    "            tmp_list.append(token.head)\n",
    "            tmp_list = sorted(tmp_list,key = lambda item:item.i)\n",
    "            if len(tmp_list)>1:\n",
    "                # print('DOBJ:',tmp_list,[item.lemma_ for item in tmp_list],[item.pos_ for item in tmp_list],[tmp_list[0].i,tmp_list[-1].i])\n",
    "                pass\n",
    "\n",
    "            if  tmp_list[0].pos_ == 'VERB' and tmp_list[-1].pos_ in ['NOUN','PROPN','PRON']:\n",
    "                verb_list = [item for item in tmp_list[0].conjuncts if item.pos_ == 'VERB'] + [tmp_list[0]]\n",
    "                noun_list = [tmp_list[-1]] + list(tmp_list[-1].conjuncts)\n",
    "                noun_list = [list(filter(lambda item:item.i <= token.i, [child for child in token.children])) + [token] for token in noun_list]\n",
    "                # print(verb_list, noun_list)\n",
    "                result_list = []\n",
    "                for verb in verb_list:\n",
    "                    for noun in noun_list:\n",
    "                        result_list.append([verb] + noun)\n",
    "                for item in result_list:\n",
    "                    # print(item, [token.pos_ for token in item], [item[0].i, item[-1].i])\n",
    "                    features['DOBJ'].append([tuple(token.lemma_ for token in item),\n",
    "                                       tuple(token.pos_ for token in item),\n",
    "                                       tuple(token.i for token in item)])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:26:07.019734Z",
     "iopub.status.busy": "2022-08-10T09:26:07.017469Z",
     "iopub.status.idle": "2022-08-10T09:26:07.056980Z",
     "shell.execute_reply": "2022-08-10T09:26:07.055982Z",
     "shell.execute_reply.started": "2022-08-10T09:26:07.019695Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_conj(feature_list):\n",
    "    words, pos, idxs = feature_list\n",
    "    result_list = []\n",
    "    for k, g in groupby(enumerate(idxs), lambda ix : ix[0] - ix[1]):\n",
    "        idx = list(map(itemgetter(0), g))\n",
    "        start = idx[0]\n",
    "        end = idx[-1] + 1\n",
    "        result_list.append([words[start:end], pos[start:end], idxs[start:end]])\n",
    "    return result_list\n",
    "\n",
    "def pdobj(features):\n",
    "    pobj = features['POBJ']\n",
    "    dobj = features['DOBJ']\n",
    "    for pitem in pobj:\n",
    "        pitem_idx = pitem[2]\n",
    "        for ditem in dobj:\n",
    "            ditem_idx = ditem[2]\n",
    "            if pitem_idx[0] == ditem_idx[0] and (set(pitem_idx).issubset(ditem_idx) or set(ditem_idx).issubset(pitem_idx)) == False:\n",
    "                # print(pitem_idx, ditem_idx)\n",
    "                new_ditem = [list(a)+list(b[1:]) for a,b in zip(pitem, ditem)]\n",
    "                x_list,y_list,z_list = [],[],[]\n",
    "                for z,x,y in sorted(zip(new_ditem[2], new_ditem[0], new_ditem[1])):\n",
    "                    if z not in z_list:\n",
    "                        x_list.append(x)\n",
    "                        y_list.append(y)\n",
    "                        z_list.append(z)\n",
    "                new_ditem_sort = [tuple(x_list), tuple(y_list), tuple(z_list)]\n",
    "                features['PDOBJ'] = features.get('PDOBJ',[])\n",
    "                features['PDOBJ'].append(new_ditem_sort)\n",
    "    return features\n",
    "\n",
    "def delete_start_stop(feature_item):\n",
    "    w,p,i = feature_item\n",
    "    w = tuple(map(lambda x: x.lower(),w))\n",
    "    if not (set(w) - MY_STOP_WORDS):\n",
    "        return None\n",
    "    for idx,word in enumerate(w):\n",
    "        if word not in MY_STOP_WORDS:\n",
    "            new_item = [w[idx:],p[idx:],i[idx:]]\n",
    "            break\n",
    "    return new_item\n",
    "\n",
    "def delete_features(features):\n",
    "    candidate_list = []\n",
    "    delete_list = []\n",
    "    for item in features.values():\n",
    "        for candidate in item:\n",
    "            clean_candidate = delete_start_stop(candidate)\n",
    "            if clean_candidate:\n",
    "                candidate_list.append(clean_candidate)\n",
    "    for item in itertools.combinations(candidate_list, 2):\n",
    "        if set(item[0][2]).issubset(item[1][2]):\n",
    "            delete_list.append(item[0])\n",
    "        elif set(item[1][2]).issubset(item[0][2]):\n",
    "            delete_list.append(item[1])\n",
    "\n",
    "    new_features = []\n",
    "    for value in candidate_list:\n",
    "        if (len(value[1]) == 1) and (value[1] != ('VERB')):\n",
    "            continue\n",
    "        if value not in delete_list:\n",
    "            new_features.append(value)\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:26:51.498172Z",
     "iopub.status.busy": "2022-08-10T09:26:51.497731Z",
     "iopub.status.idle": "2022-08-10T09:26:51.508654Z",
     "shell.execute_reply": "2022-08-10T09:26:51.507153Z",
     "shell.execute_reply.started": "2022-08-10T09:26:51.498137Z"
    }
   },
   "outputs": [],
   "source": [
    "def fiter(lines):\n",
    "    new_lines = []\n",
    "    for i in lines:\n",
    "        if len(i[0])>60 and len(i[0])<600 and \"Incredible\" not in i[0] and i not in new_lines:\n",
    "            new_lines.append(i)\n",
    "    return new_lines\n",
    "def patern(test_lines,name):\n",
    "    json_list = []\n",
    "    for ridx,line in enumerate(test_lines):\n",
    "        sents = nltk.sent_tokenize(line[0])\n",
    "        for sidx,sent in enumerate(sents):\n",
    "            result_dict = {'review_id':ridx,\n",
    "                            'sent_id':sidx,\n",
    "                            'sent':sent,}\n",
    "            features = syntactic_match(sent)\n",
    "\n",
    "            new_cc_list = []\n",
    "            for item in features['CC_NOUN']:\n",
    "                new_cc_list.extend(split_conj(item))\n",
    "            features['CC_NOUN'] = new_cc_list\n",
    "\n",
    "            features = pdobj(features)\n",
    "            new_features = delete_features(features)\n",
    "            result_dict['features'] = new_features\n",
    "            json_list.append(result_dict)\n",
    "    dumpJson('./{}.json'.format(name),json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T07:26:21.168092Z",
     "iopub.status.busy": "2022-08-05T07:26:21.167153Z",
     "iopub.status.idle": "2022-08-05T07:29:08.885007Z",
     "shell.execute_reply": "2022-08-05T07:29:08.883706Z",
     "shell.execute_reply.started": "2022-08-05T07:26:21.168043Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed('data')\n",
    "lines = readCsvToList('../input/privacy-datas/Privacy_data/Instagram_privacy.csv')\n",
    "app_list = [\n",
    "\"Facebook_privacy\",\n",
    "\"Instagram_privacy\",\n",
    "\"Spotify_privacy\",\n",
    "\"TikTok_privacy\",\n",
    "\"Twitter_privacy\",\n",
    "\"YouTube_privacy\",\n",
    "\"pocketguard\"\n",
    "]\n",
    "\n",
    "for i in app_list:\n",
    "    lines = readCsvToList('../input/privacy-datas/Privacy_data/{}.csv'.format(i))\n",
    "    a = fiter(lines)[:500]\n",
    "    random.shuffle(a)\n",
    "    test_lines = a[:50]\n",
    "    patern(test_lines,i+'_syntactic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
